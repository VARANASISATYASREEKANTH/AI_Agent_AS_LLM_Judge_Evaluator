# AI_Agent_AS_LLM_Judge_Evaluator
Using AI Agent as LLM Judge Evaluator
<table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Evaluation Metric</th>
      <th>Methodology(LLM/AI Agent/ Multi-Agent)</th>
      <th>Strategy</th>
      <th>Remarks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Exact Match</td>
      <td>User login &amp; signup</td>
      <td>‚úÖ Done</td>
      <td>Supports OAuth</td>
    </tr>
    <tr>
      <td>BLEU(avg)</td>
      <td>Main UI components</td>
      <td>üõ† In Progress</td>
      <td>Needs polishing</td>
    </tr>
    <tr>
      <td>ROUGE L(avg)</td>
      <td>RESTful endpoints</td>
      <td>‚úÖ Done</td>
      <td>Version 1.0 stable</td>
    </tr>
    <tr>
      <td>Cosine similarity</td>
      <td>CI/CD Integration</td>
      <td>‚ùå Pending</td>
      <td>Needs server config</td>
    </tr>
    <tr>
      <td>BERT-f_1 score</td>
      <td>CI/CD Integration</td>
      <td>‚ùå Pending</td>
      <td>Needs server config</td>
    </tr>


    
  </tbody>
</table>
