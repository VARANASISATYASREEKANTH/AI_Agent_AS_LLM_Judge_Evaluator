{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f6be85",
   "metadata": {},
   "source": [
    "## Evalauting LLM in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEKANTHVS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEKANTHVS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import json\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# === Load the GGUF Model ===\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"C:/myworks/MODELS\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    max_new_tokens=200,\n",
    "    threads=4\n",
    ")\n",
    "\n",
    "# === Sample Evaluation Data ===\n",
    "eval_data = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What is the chemical formula of water?\", \"answer\": \"H2O\"},\n",
    "]\n",
    "\n",
    "# === Embedding Model for Cosine Similarity ===\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === ROUGE & BLEU Settings ===\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# === Store Results ===\n",
    "results = []\n",
    "all_preds = []\n",
    "all_refs = []\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "for item in eval_data:\n",
    "    prompt = f\"### Question: {item['question']}\\n### Answer:\"\n",
    "    prediction = model(prompt).strip()\n",
    "    reference = item['answer'].strip()\n",
    "\n",
    "    # Save for batch metrics\n",
    "    all_preds.append(prediction)\n",
    "    all_refs.append(reference)\n",
    "\n",
    "    # === Metrics ===\n",
    "    exact_match = prediction.lower().strip() == reference.lower().strip()\n",
    "    bleu = sentence_bleu([nltk.word_tokenize(reference)], nltk.word_tokenize(prediction), smoothing_function=smoothie)\n",
    "    rouge_l = rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "    cosine = cosine_similarity(embedder.encode([reference]), embedder.encode([prediction]))[0][0]\n",
    "\n",
    "    results.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"bleu\": bleu,\n",
    "        \"rougeL\": rouge_l,\n",
    "        \"cosine_similarity\": cosine\n",
    "    })\n",
    "\n",
    "# === BERTScore (Batch) ===\n",
    "P, R, F1 = bert_score(all_preds, all_refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "# === Aggregate Metrics ===\n",
    "aggregate = {\n",
    "    \"exact_match (%)\": round(np.mean([r[\"exact_match\"] for r in results]) * 100, 2),\n",
    "    \"bleu (avg)\": round(np.mean([r[\"bleu\"] for r in results]), 4),\n",
    "    \"rougeL (avg)\": round(np.mean([r[\"rougeL\"] for r in results]), 4),\n",
    "    \"cosine_similarity (avg)\": round(np.mean([r[\"cosine_similarity\"] for r in results]), 4),\n",
    "    \"bert_score_f1 (avg)\": round(float(F1.mean()), 4)\n",
    "}\n",
    "\n",
    "# === Save to File ===\n",
    "with open(\"C:/myworks/AI_Agent_AS_LLM_Judge_Evaluator/llm_eval_full_metrics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Detailed Results ===\\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "    f.write(\"\\n\\n=== Aggregated Metrics ===\\n\")\n",
    "    f.write(json.dumps(aggregate, indent=2))\n",
    "\n",
    "print(\"‚úÖ Evaluation Complete. Summary:\")\n",
    "print(json.dumps(aggregate, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9f4ff",
   "metadata": {},
   "source": [
    "## AI Agent as LLM Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa82b5a",
   "metadata": {},
   "source": [
    "###  Let‚Äôs now turn the LLM Judge into an AI Agent ‚Äì one that acts autonomously to evaluate candidate answers against reference answers with chain-of-thought reasoning and decision logic.\n",
    "\n",
    "An AI agent LLM Judge:\n",
    "\n",
    "Uses a deliberate step-by-step reasoning chain.\n",
    "\n",
    "Follows rules (like rubric-based scoring).\n",
    "\n",
    "May critique, score, and even revise evaluations if they are inconsistent.\n",
    "\n",
    "Can be reused across multiple tasks ‚Äî like grading, summarizing, or ranking.\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Prompt     ‚îÇ\n",
    "‚îÇ Reference  ‚îÇ\n",
    "‚îÇ Candidate  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ AI Agent Judge (LLM pipeline)‚îÇ\n",
    "‚îÇ - Step 1: Understand intent   ‚îÇ\n",
    "‚îÇ - Step 2: Compare answers     ‚îÇ\n",
    "‚îÇ - Step 3: Reason on quality   ‚îÇ\n",
    "‚îÇ - Step 4: Generate judgment   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "     Score, Explanation, Critique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import bert_score\n",
    "import numpy as np\n",
    "\n",
    "# Load LLM\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Or another instruct model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "judge = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Dataset\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the greenhouse effect.\",\n",
    "        \"reference\": \"The greenhouse effect traps heat in Earth's atmosphere due to gases like CO2 and methane.\",\n",
    "        \"candidate\": \"The atmosphere traps heat because of gases like carbon dioxide, causing Earth to warm up.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes deforestation?\",\n",
    "        \"reference\": \"Deforestation is caused by logging, farming, mining, and urban sprawl.\",\n",
    "        \"candidate\": \"People cut trees for farming and for building towns.\"\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Agent Prompt Template\n",
    "def agent_prompt(prompt, reference, candidate):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluation AI agent.\n",
    "\n",
    "You will follow a step-by-step reasoning process to evaluate how well a candidate answer responds to a prompt compared to a reference answer. Your steps:\n",
    "\n",
    "Step 1: Restate the intent of the prompt.\n",
    "Step 2: Summarize the reference answer concisely.\n",
    "Step 3: Summarize the candidate answer.\n",
    "Step 4: Identify key differences.\n",
    "Step 5: Judge the candidate's relevance, accuracy, and completeness compared to the reference.\n",
    "Step 6: Assign a score from 1 (poor) to 5 (excellent).\n",
    "Step 7: Provide a concise explanation for the score.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "REFERENCE ANSWER: {reference}\n",
    "CANDIDATE ANSWER: {candidate}\n",
    "\n",
    "Respond using this format:\n",
    "Intent: ...\n",
    "Reference Summary: ...\n",
    "Candidate Summary: ...\n",
    "Comparison: ...\n",
    "Evaluation: ...\n",
    "Score: <1-5>\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_sim(text1, text2):\n",
    "    v1 = embedder.encode([text1])[0]\n",
    "    v2 = embedder.encode([text2])[0]\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Score extractor\n",
    "def extract_score(agent_output):\n",
    "    try:\n",
    "        for line in agent_output.split('\\n'):\n",
    "            if \"score\" in line.lower():\n",
    "                digits = ''.join(filter(str.isdigit, line))\n",
    "                return int(digits) if digits else None\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Run the AI agent\n",
    "results = []\n",
    "all_references, all_candidates = [], []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"Evaluating #{idx+1}\")\n",
    "    prompt_text = agent_prompt(row[\"prompt\"], row[\"reference\"], row[\"candidate\"])\n",
    "    output = judge(prompt_text)[0][\"generated_text\"]\n",
    "    score = extract_score(output)\n",
    "    sim = cosine_sim(row[\"reference\"], row[\"candidate\"])\n",
    "    \n",
    "    all_references.append(row[\"reference\"])\n",
    "    all_candidates.append(row[\"candidate\"])\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"score\": score,\n",
    "        \"cosine_similarity\": sim,\n",
    "        \"agent_output\": output\n",
    "    })\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = bert_score.score(all_candidates, all_references, lang=\"en\", verbose=False)\n",
    "for i, f1 in enumerate(F1):\n",
    "    results[i][\"bertscore_f1\"] = f1.item()\n",
    "\n",
    "# Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[[\"prompt\", \"score\", \"cosine_similarity\", \"bertscore_f1\"]])\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Summary Metrics:\")\n",
    "print(f\"Avg Score: {results_df['score'].mean():.2f}\")\n",
    "print(f\"Avg Cosine Similarity: {results_df['cosine_similarity'].mean():.2f}\")\n",
    "print(f\"Avg BERTScore F1: {results_df['bertscore_f1'].mean():.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.hist(results_df[\"score\"].dropna(), bins=[1,2,3,4,5,6], edgecolor=\"black\")\n",
    "plt.title(\"AI Agent Judge Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98e14e",
   "metadata": {},
   "source": [
    "# Multi Agent voting\n",
    "\n",
    " AI Judge Agent with Multi-Agent Voting, where multiple LLMs act as judges and vote on the score. Then we‚Äôll:\n",
    "\n",
    "üß† Aggregate their scores (majority vote or average)\n",
    "\n",
    "üìù Save all judge outputs, scores, and metrics to a results.txt file\n",
    "\n",
    "| Feature                 | Implemented? |\n",
    "| ----------------------- | ------------ |\n",
    "| AI Agent LLM Judge      | ‚úÖ Yes        |\n",
    "| Multi-Model Judging     | ‚úÖ Yes        |\n",
    "| Voting/Averaging Scores | ‚úÖ Yes        |\n",
    "| Save Results to File    | ‚úÖ Yes        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import bert_score\n",
    "\n",
    "# Load AI judge models (you can expand this list)\n",
    "JUDGE_MODELS = {\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"LLaMA\": \"meta-llama/Llama-2-7b-chat-hf\"  # swap this with another local model if needed\n",
    "}\n",
    "\n",
    "# Load models and tokenizers\n",
    "judges = {}\n",
    "for name, model_id in JUDGE_MODELS.items():\n",
    "    print(f\"Loading judge model: {name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    judges[name] = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the greenhouse effect.\",\n",
    "        \"reference\": \"The greenhouse effect traps heat in Earth's atmosphere due to gases like CO2 and methane.\",\n",
    "        \"candidate\": \"The atmosphere traps heat because of gases like carbon dioxide, causing Earth to warm up.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes deforestation?\",\n",
    "        \"reference\": \"Deforestation is caused by logging, farming, mining, and urban sprawl.\",\n",
    "        \"candidate\": \"People cut trees for farming and for building towns.\"\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Format prompt for agents\n",
    "def agent_prompt(prompt, reference, candidate):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator agent.\n",
    "\n",
    "Step 1: Understand the prompt.\n",
    "Step 2: Summarize the reference and candidate.\n",
    "Step 3: Compare candidate to reference.\n",
    "Step 4: Judge accuracy, completeness, and clarity.\n",
    "Step 5: Score from 1 (poor) to 5 (excellent).\n",
    "Step 6: Explain the score.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "REFERENCE: {reference}\n",
    "CANDIDATE: {candidate}\n",
    "\n",
    "Respond using:\n",
    "Intent: ...\n",
    "Reference Summary: ...\n",
    "Candidate Summary: ...\n",
    "Comparison: ...\n",
    "Score: <1-5>\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "\n",
    "# Score extractor\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        for line in output_text.split(\"\\n\"):\n",
    "            if \"score\" in line.lower():\n",
    "                digits = ''.join(filter(str.isdigit, line))\n",
    "                return int(digits) if digits else None\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_sim(text1, text2):\n",
    "    v1 = embedder.encode([text1])[0]\n",
    "    v2 = embedder.encode([text2])[0]\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "all_refs, all_cands = [], []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"\\nEvaluating Sample #{idx+1}\")\n",
    "    prompt_text = agent_prompt(row[\"prompt\"], row[\"reference\"], row[\"candidate\"])\n",
    "    all_refs.append(row[\"reference\"])\n",
    "    all_cands.append(row[\"candidate\"])\n",
    "    \n",
    "    # Judge responses and scores\n",
    "    judge_outputs = {}\n",
    "    scores = []\n",
    "    \n",
    "    for judge_name, judge_pipe in judges.items():\n",
    "        try:\n",
    "            output = judge_pipe(prompt_text)[0][\"generated_text\"]\n",
    "            score = extract_score(output)\n",
    "            scores.append(score)\n",
    "            judge_outputs[judge_name] = {\"score\": score, \"explanation\": output}\n",
    "        except Exception as e:\n",
    "            judge_outputs[judge_name] = {\"score\": None, \"explanation\": str(e)}\n",
    "    \n",
    "    # Voting: average score\n",
    "    valid_scores = [s for s in scores if s is not None]\n",
    "    avg_score = np.mean(valid_scores) if valid_scores else None\n",
    "    sim = cosine_sim(row[\"reference\"], row[\"candidate\"])\n",
    "    \n",
    "    results.append({\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"candidate\": row[\"candidate\"],\n",
    "        \"average_score\": avg_score,\n",
    "        \"cosine_similarity\": sim,\n",
    "        \"judge_outputs\": judge_outputs\n",
    "    })\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score.score(all_cands, all_refs, lang=\"en\", verbose=False)\n",
    "for i in range(len(results)):\n",
    "    results[i][\"bertscore_f1\"] = F1[i].item()\n",
    "\n",
    "# Save results to text file\n",
    "with open(\"llm_evaluation_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, entry in enumerate(results):\n",
    "        f.write(f\"\\n### Evaluation Sample {i+1} ###\\n\")\n",
    "        f.write(f\"Prompt: {entry['prompt']}\\n\")\n",
    "        f.write(f\"Candidate Answer: {entry['candidate']}\\n\")\n",
    "        f.write(f\"Cosine Similarity: {entry['cosine_similarity']:.3f}\\n\")\n",
    "        f.write(f\"BERTScore F1: {entry['bertscore_f1']:.3f}\\n\")\n",
    "        f.write(f\"Average Judge Score: {entry['average_score']:.2f}\\n\")\n",
    "        f.write(f\"\\n--- Judge Responses ---\\n\")\n",
    "        for judge_name, response in entry[\"judge_outputs\"].items():\n",
    "            f.write(f\"\\n[{judge_name}]\\n\")\n",
    "            f.write(f\"Score: {response['score']}\\n\")\n",
    "            f.write(\"Explanation:\\n\")\n",
    "            f.write(response[\"explanation\"] + \"\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ All results saved to `llm_evaluation_results.txt`.\")\n",
    "\n",
    "# Summary (optional visualization)\n",
    "scores = [r[\"average_score\"] for r in results if r[\"average_score\"] is not None]\n",
    "plt.hist(scores, bins=[1,2,3,4,5,6], edgecolor=\"black\")\n",
    "plt.title(\"Multi-Agent Score Distribution\")\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b46118",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
