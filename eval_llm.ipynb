{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f6be85",
   "metadata": {},
   "source": [
    "## Evalauting LLM in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEKANTHVS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SREEKANTHVS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import json\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# === Load the GGUF Model ===\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"C:/myworks/MODELS\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    max_new_tokens=200,\n",
    "    threads=4\n",
    ")\n",
    "\n",
    "# === Sample Evaluation Data ===\n",
    "eval_data = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What is the chemical formula of water?\", \"answer\": \"H2O\"},\n",
    "]\n",
    "\n",
    "# === Embedding Model for Cosine Similarity ===\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === ROUGE & BLEU Settings ===\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# === Store Results ===\n",
    "results = []\n",
    "all_preds = []\n",
    "all_refs = []\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "for item in eval_data:\n",
    "    prompt = f\"### Question: {item['question']}\\n### Answer:\"\n",
    "    prediction = model(prompt).strip()\n",
    "    reference = item['answer'].strip()\n",
    "\n",
    "    # Save for batch metrics\n",
    "    all_preds.append(prediction)\n",
    "    all_refs.append(reference)\n",
    "\n",
    "    # === Metrics ===\n",
    "    exact_match = prediction.lower().strip() == reference.lower().strip()\n",
    "    bleu = sentence_bleu([nltk.word_tokenize(reference)], nltk.word_tokenize(prediction), smoothing_function=smoothie)\n",
    "    rouge_l = rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "    cosine = cosine_similarity(embedder.encode([reference]), embedder.encode([prediction]))[0][0]\n",
    "\n",
    "    results.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"bleu\": bleu,\n",
    "        \"rougeL\": rouge_l,\n",
    "        \"cosine_similarity\": cosine\n",
    "    })\n",
    "\n",
    "# === BERTScore (Batch) ===\n",
    "P, R, F1 = bert_score(all_preds, all_refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "# === Aggregate Metrics ===\n",
    "aggregate = {\n",
    "    \"exact_match (%)\": round(np.mean([r[\"exact_match\"] for r in results]) * 100, 2),\n",
    "    \"bleu (avg)\": round(np.mean([r[\"bleu\"] for r in results]), 4),\n",
    "    \"rougeL (avg)\": round(np.mean([r[\"rougeL\"] for r in results]), 4),\n",
    "    \"cosine_similarity (avg)\": round(np.mean([r[\"cosine_similarity\"] for r in results]), 4),\n",
    "    \"bert_score_f1 (avg)\": round(float(F1.mean()), 4)\n",
    "}\n",
    "\n",
    "# === Save to File ===\n",
    "with open(\"C:/myworks/AI_Agent_AS_LLM_Judge_Evaluator/llm_eval_full_metrics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Detailed Results ===\\n\")\n",
    "    f.write(json.dumps(results, indent=2))\n",
    "    f.write(\"\\n\\n=== Aggregated Metrics ===\\n\")\n",
    "    f.write(json.dumps(aggregate, indent=2))\n",
    "\n",
    "print(\"✅ Evaluation Complete. Summary:\")\n",
    "print(json.dumps(aggregate, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9f4ff",
   "metadata": {},
   "source": [
    "## AI Agent as LLM Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa82b5a",
   "metadata": {},
   "source": [
    "###  Let’s now turn the LLM Judge into an AI Agent – one that acts autonomously to evaluate candidate answers against reference answers with chain-of-thought reasoning and decision logic.\n",
    "\n",
    "An AI agent LLM Judge:\n",
    "\n",
    "Uses a deliberate step-by-step reasoning chain.\n",
    "\n",
    "Follows rules (like rubric-based scoring).\n",
    "\n",
    "May critique, score, and even revise evaluations if they are inconsistent.\n",
    "\n",
    "Can be reused across multiple tasks — like grading, summarizing, or ranking.\n",
    "┌────────────┐\n",
    "│ Prompt     │\n",
    "│ Reference  │\n",
    "│ Candidate  │\n",
    "└─────┬──────┘\n",
    "      │\n",
    "      ▼\n",
    "┌───────────────────────────────┐\n",
    "│ AI Agent Judge (LLM pipeline)│\n",
    "│ - Step 1: Understand intent   │\n",
    "│ - Step 2: Compare answers     │\n",
    "│ - Step 3: Reason on quality   │\n",
    "│ - Step 4: Generate judgment   │\n",
    "└────────────┬──────────────────┘\n",
    "             │\n",
    "             ▼\n",
    "     Score, Explanation, Critique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import bert_score\n",
    "import numpy as np\n",
    "\n",
    "# Load LLM\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Or another instruct model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "judge = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Dataset\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the greenhouse effect.\",\n",
    "        \"reference\": \"The greenhouse effect traps heat in Earth's atmosphere due to gases like CO2 and methane.\",\n",
    "        \"candidate\": \"The atmosphere traps heat because of gases like carbon dioxide, causing Earth to warm up.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes deforestation?\",\n",
    "        \"reference\": \"Deforestation is caused by logging, farming, mining, and urban sprawl.\",\n",
    "        \"candidate\": \"People cut trees for farming and for building towns.\"\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Agent Prompt Template\n",
    "def agent_prompt(prompt, reference, candidate):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluation AI agent.\n",
    "\n",
    "You will follow a step-by-step reasoning process to evaluate how well a candidate answer responds to a prompt compared to a reference answer. Your steps:\n",
    "\n",
    "Step 1: Restate the intent of the prompt.\n",
    "Step 2: Summarize the reference answer concisely.\n",
    "Step 3: Summarize the candidate answer.\n",
    "Step 4: Identify key differences.\n",
    "Step 5: Judge the candidate's relevance, accuracy, and completeness compared to the reference.\n",
    "Step 6: Assign a score from 1 (poor) to 5 (excellent).\n",
    "Step 7: Provide a concise explanation for the score.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "REFERENCE ANSWER: {reference}\n",
    "CANDIDATE ANSWER: {candidate}\n",
    "\n",
    "Respond using this format:\n",
    "Intent: ...\n",
    "Reference Summary: ...\n",
    "Candidate Summary: ...\n",
    "Comparison: ...\n",
    "Evaluation: ...\n",
    "Score: <1-5>\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_sim(text1, text2):\n",
    "    v1 = embedder.encode([text1])[0]\n",
    "    v2 = embedder.encode([text2])[0]\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Score extractor\n",
    "def extract_score(agent_output):\n",
    "    try:\n",
    "        for line in agent_output.split('\\n'):\n",
    "            if \"score\" in line.lower():\n",
    "                digits = ''.join(filter(str.isdigit, line))\n",
    "                return int(digits) if digits else None\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Run the AI agent\n",
    "results = []\n",
    "all_references, all_candidates = [], []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"Evaluating #{idx+1}\")\n",
    "    prompt_text = agent_prompt(row[\"prompt\"], row[\"reference\"], row[\"candidate\"])\n",
    "    output = judge(prompt_text)[0][\"generated_text\"]\n",
    "    score = extract_score(output)\n",
    "    sim = cosine_sim(row[\"reference\"], row[\"candidate\"])\n",
    "    \n",
    "    all_references.append(row[\"reference\"])\n",
    "    all_candidates.append(row[\"candidate\"])\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"score\": score,\n",
    "        \"cosine_similarity\": sim,\n",
    "        \"agent_output\": output\n",
    "    })\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = bert_score.score(all_candidates, all_references, lang=\"en\", verbose=False)\n",
    "for i, f1 in enumerate(F1):\n",
    "    results[i][\"bertscore_f1\"] = f1.item()\n",
    "\n",
    "# Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[[\"prompt\", \"score\", \"cosine_similarity\", \"bertscore_f1\"]])\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary Metrics:\")\n",
    "print(f\"Avg Score: {results_df['score'].mean():.2f}\")\n",
    "print(f\"Avg Cosine Similarity: {results_df['cosine_similarity'].mean():.2f}\")\n",
    "print(f\"Avg BERTScore F1: {results_df['bertscore_f1'].mean():.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.hist(results_df[\"score\"].dropna(), bins=[1,2,3,4,5,6], edgecolor=\"black\")\n",
    "plt.title(\"AI Agent Judge Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98e14e",
   "metadata": {},
   "source": [
    "# Multi Agent voting\n",
    "\n",
    " AI Judge Agent with Multi-Agent Voting, where multiple LLMs act as judges and vote on the score. Then we’ll:\n",
    "\n",
    "🧠 Aggregate their scores (majority vote or average)\n",
    "\n",
    "📝 Save all judge outputs, scores, and metrics to a results.txt file\n",
    "\n",
    "| Feature                 | Implemented? |\n",
    "| ----------------------- | ------------ |\n",
    "| AI Agent LLM Judge      | ✅ Yes        |\n",
    "| Multi-Model Judging     | ✅ Yes        |\n",
    "| Voting/Averaging Scores | ✅ Yes        |\n",
    "| Save Results to File    | ✅ Yes        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import bert_score\n",
    "\n",
    "# Load AI judge models (you can expand this list)\n",
    "JUDGE_MODELS = {\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"LLaMA\": \"meta-llama/Llama-2-7b-chat-hf\"  # swap this with another local model if needed\n",
    "}\n",
    "\n",
    "# Load models and tokenizers\n",
    "judges = {}\n",
    "for name, model_id in JUDGE_MODELS.items():\n",
    "    print(f\"Loading judge model: {name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    judges[name] = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the greenhouse effect.\",\n",
    "        \"reference\": \"The greenhouse effect traps heat in Earth's atmosphere due to gases like CO2 and methane.\",\n",
    "        \"candidate\": \"The atmosphere traps heat because of gases like carbon dioxide, causing Earth to warm up.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes deforestation?\",\n",
    "        \"reference\": \"Deforestation is caused by logging, farming, mining, and urban sprawl.\",\n",
    "        \"candidate\": \"People cut trees for farming and for building towns.\"\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Format prompt for agents\n",
    "def agent_prompt(prompt, reference, candidate):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator agent.\n",
    "\n",
    "Step 1: Understand the prompt.\n",
    "Step 2: Summarize the reference and candidate.\n",
    "Step 3: Compare candidate to reference.\n",
    "Step 4: Judge accuracy, completeness, and clarity.\n",
    "Step 5: Score from 1 (poor) to 5 (excellent).\n",
    "Step 6: Explain the score.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "REFERENCE: {reference}\n",
    "CANDIDATE: {candidate}\n",
    "\n",
    "Respond using:\n",
    "Intent: ...\n",
    "Reference Summary: ...\n",
    "Candidate Summary: ...\n",
    "Comparison: ...\n",
    "Score: <1-5>\n",
    "Explanation: ...\n",
    "\"\"\"\n",
    "\n",
    "# Score extractor\n",
    "def extract_score(output_text):\n",
    "    try:\n",
    "        for line in output_text.split(\"\\n\"):\n",
    "            if \"score\" in line.lower():\n",
    "                digits = ''.join(filter(str.isdigit, line))\n",
    "                return int(digits) if digits else None\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_sim(text1, text2):\n",
    "    v1 = embedder.encode([text1])[0]\n",
    "    v2 = embedder.encode([text2])[0]\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "all_refs, all_cands = [], []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"\\nEvaluating Sample #{idx+1}\")\n",
    "    prompt_text = agent_prompt(row[\"prompt\"], row[\"reference\"], row[\"candidate\"])\n",
    "    all_refs.append(row[\"reference\"])\n",
    "    all_cands.append(row[\"candidate\"])\n",
    "    \n",
    "    # Judge responses and scores\n",
    "    judge_outputs = {}\n",
    "    scores = []\n",
    "    \n",
    "    for judge_name, judge_pipe in judges.items():\n",
    "        try:\n",
    "            output = judge_pipe(prompt_text)[0][\"generated_text\"]\n",
    "            score = extract_score(output)\n",
    "            scores.append(score)\n",
    "            judge_outputs[judge_name] = {\"score\": score, \"explanation\": output}\n",
    "        except Exception as e:\n",
    "            judge_outputs[judge_name] = {\"score\": None, \"explanation\": str(e)}\n",
    "    \n",
    "    # Voting: average score\n",
    "    valid_scores = [s for s in scores if s is not None]\n",
    "    avg_score = np.mean(valid_scores) if valid_scores else None\n",
    "    sim = cosine_sim(row[\"reference\"], row[\"candidate\"])\n",
    "    \n",
    "    results.append({\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"candidate\": row[\"candidate\"],\n",
    "        \"average_score\": avg_score,\n",
    "        \"cosine_similarity\": sim,\n",
    "        \"judge_outputs\": judge_outputs\n",
    "    })\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score.score(all_cands, all_refs, lang=\"en\", verbose=False)\n",
    "for i in range(len(results)):\n",
    "    results[i][\"bertscore_f1\"] = F1[i].item()\n",
    "\n",
    "# Save results to text file\n",
    "with open(\"llm_evaluation_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, entry in enumerate(results):\n",
    "        f.write(f\"\\n### Evaluation Sample {i+1} ###\\n\")\n",
    "        f.write(f\"Prompt: {entry['prompt']}\\n\")\n",
    "        f.write(f\"Candidate Answer: {entry['candidate']}\\n\")\n",
    "        f.write(f\"Cosine Similarity: {entry['cosine_similarity']:.3f}\\n\")\n",
    "        f.write(f\"BERTScore F1: {entry['bertscore_f1']:.3f}\\n\")\n",
    "        f.write(f\"Average Judge Score: {entry['average_score']:.2f}\\n\")\n",
    "        f.write(f\"\\n--- Judge Responses ---\\n\")\n",
    "        for judge_name, response in entry[\"judge_outputs\"].items():\n",
    "            f.write(f\"\\n[{judge_name}]\\n\")\n",
    "            f.write(f\"Score: {response['score']}\\n\")\n",
    "            f.write(\"Explanation:\\n\")\n",
    "            f.write(response[\"explanation\"] + \"\\n\")\n",
    "\n",
    "print(\"\\n✅ All results saved to `llm_evaluation_results.txt`.\")\n",
    "\n",
    "# Summary (optional visualization)\n",
    "scores = [r[\"average_score\"] for r in results if r[\"average_score\"] is not None]\n",
    "plt.hist(scores, bins=[1,2,3,4,5,6], edgecolor=\"black\")\n",
    "plt.title(\"Multi-Agent Score Distribution\")\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b46118",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
